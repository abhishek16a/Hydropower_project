{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8475d157",
   "metadata": {},
   "source": [
    "# 03 – Unstructured Text → Daily Topic/Keyword Features (DHM Notices & News)\n",
    "**Goal:** Crawl DHM `/notice` and `/news` pages, fetch detail pages, parse text, build keyword features (English+Nepali),\n",
    "and merge with master daily data (2019–2023).\n",
    "\n",
    "Steps:\n",
    "1. Crawl listing pages (notice/news) with pagination.\n",
    "2. Collect article links, fetch detail pages, extract date/title/body.\n",
    "3. Filter to 2019–2023, dedupe, save `text_corpus.csv`.\n",
    "4. Keyword features → `topics_daily.csv`.\n",
    "5. Optional LDA topics.\n",
    "6. Merge with `master_kaligandaki_daily_withrain.csv` → `master_with_topics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef46f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "BASE = 'https://www.dhm.gov.np'\n",
    "LISTING_ENDPOINTS = ['notice', 'news']\n",
    "MAX_PAGES_PER_SECTION = 10\n",
    "UA = {'User-Agent': 'Mozilla/5.0'}\n",
    "TIMEOUT = 30\n",
    "DELAY = 1.0\n",
    "\n",
    "TEXT_CORPUS_CSV = 'text_corpus.csv'\n",
    "TOPICS_DAILY_CSV = 'topics_daily.csv'\n",
    "MASTER_PATH = 'master_kaligandaki_daily_withrain.csv'\n",
    "MASTER_WITH_TOPICS = 'master_with_topics.csv'\n",
    "START_DATE = pd.Timestamp('2019-01-01')\n",
    "END_DATE = pd.Timestamp('2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=UA, timeout=TIMEOUT)\n",
    "        if r.status_code == 200 and r.text:\n",
    "            time.sleep(DELAY)\n",
    "            return r.text\n",
    "    except Exception as e:\n",
    "        print('Error fetching', url, e)\n",
    "    return ''\n",
    "\n",
    "def abs_url(href):\n",
    "    return urljoin(BASE+'/', href) if href else None\n",
    "\n",
    "def safe_date(txt):\n",
    "    if not txt: return None\n",
    "    try:\n",
    "        return dparser.parse(txt, fuzzy=True, dayfirst=True).date()\n",
    "    except: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f915f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl listings\n",
    "article_urls = set()\n",
    "for section in LISTING_ENDPOINTS:\n",
    "    for p in range(MAX_PAGES_PER_SECTION):\n",
    "        url = f\"{BASE}/{section}?page={p}\"\n",
    "        html = get_html(url)\n",
    "        if not html: continue\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for a in soup.find_all('a'):\n",
    "            href = a.get('href','')\n",
    "            if any(f'/{s}/' in href for s in LISTING_ENDPOINTS):\n",
    "                article_urls.add(abs_url(href))\n",
    "        print(section, 'page', p, '->', len(article_urls), 'urls so far')\n",
    "\n",
    "article_urls = {u for u in article_urls if urlparse(u).netloc}\n",
    "print('Total collected URLs:', len(article_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_HINT_SEL = ['time','.date','.post-date','.posted-on','.views-field-created','.field--name-created']\n",
    "BODY_SEL = ['.field--name-body','article','.content','.entry-content','.node__content']\n",
    "\n",
    "def parse_article(url):\n",
    "    html = get_html(url)\n",
    "    if not html: return None\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    title = None\n",
    "    h = soup.find(['h1','h2','h3'])\n",
    "    if h: title = h.get_text(' ',strip=True)\n",
    "    dt = None\n",
    "    for sel in DATE_HINT_SEL:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            dt = safe_date(node.get('datetime') or node.get_text(' ',strip=True))\n",
    "            if dt: break\n",
    "    if not dt and title: dt = safe_date(title)\n",
    "    if not dt: dt = safe_date(soup.get_text(' ',strip=True))\n",
    "    body = None\n",
    "    for sel in BODY_SEL:\n",
    "        node = soup.select_one(sel)\n",
    "        if node:\n",
    "            body = node.get_text(' ',strip=True)\n",
    "            if body and len(body)>40: break\n",
    "    if not body: body = soup.get_text(' ',strip=True)\n",
    "    return {'date':pd.to_datetime(dt,errors='coerce'),'source':'DHM','title':title,'text':body,'url':url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ee1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i,url in enumerate(sorted(article_urls)):\n",
    "    rec = parse_article(url)\n",
    "    if rec and pd.notna(rec['date']):\n",
    "        records.append(rec)\n",
    "    if (i+1)%25==0: print('Parsed',i+1)\n",
    "\n",
    "corpus_df = pd.DataFrame(records)\n",
    "if not corpus_df.empty:\n",
    "    corpus_df['date'] = pd.to_datetime(corpus_df['date']).dt.normalize()\n",
    "    mask = (corpus_df['date']>=START_DATE)&(corpus_df['date']<=END_DATE)\n",
    "    corpus_df = corpus_df.loc[mask].drop_duplicates(subset=['date','title','url'])\n",
    "\n",
    "corpus_df.to_csv(TEXT_CORPUS_CSV,index=False)\n",
    "print('Saved corpus',TEXT_CORPUS_CSV,'rows=',len(corpus_df))\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bdf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = {\n",
    " 'maintenance':['maintenance','overhaul','shutdown','servicing','repair','सम्भार','मर्मत','बन्द'],\n",
    " 'outage':['outage','blackout','interruption','load shedding','trip','fault','विद्युत अवरोध','लोडसेडिङ','बत्ती बन्द'],\n",
    " 'flood':['flood','high flow','inundation','alert','warning','watch','बाढी','पहिरो','सूचना','चेतावनी'],\n",
    " 'policy':['policy','tariff','import','export','regulation','curtail','नीति','दर','आयात','निर्यात','विनियमन'],\n",
    " 'weather':['heavy rain','thunder','storm','monsoon','precipitation','हावाहुरी','मुसलधारे','मेघगर्जन','मौसम']\n",
    "}\n",
    "\n",
    "def kw_counts(text):\n",
    "    t=(text or '').lower()\n",
    "    out={k:0 for k in KEYWORDS}\n",
    "    for k,words in KEYWORDS.items():\n",
    "        for w in words:\n",
    "            out[k]+=t.count(w.lower())\n",
    "    return out\n",
    "\n",
    "kk=[]\n",
    "for _,r in corpus_df.iterrows():\n",
    "    counts=kw_counts((r.get('title') or '')+' '+(r.get('text') or ''))\n",
    "    counts['date']=r['date'].date()\n",
    "    kk.append(counts)\n",
    "\n",
    "daily_kw=pd.DataFrame(kk)\n",
    "if not daily_kw.empty:\n",
    "    daily_kw['date']=pd.to_datetime(daily_kw['date'])\n",
    "    for k in KEYWORDS:\n",
    "        daily_kw[f'{k}_flag']=(daily_kw[k]>0).astype(int)\n",
    "    daily_kw=daily_kw.groupby('date',as_index=False).sum()\n",
    "\n",
    "daily_kw.to_csv(TOPICS_DAILY_CSV,index=False)\n",
    "print('Saved',TOPICS_DAILY_CSV,'rows=',len(daily_kw))\n",
    "daily_kw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "master=pd.read_csv(MASTER_PATH,parse_dates=['date'])\n",
    "out=master.merge(daily_kw,on='date',how='left')\n",
    "for c in out.columns:\n",
    "    if c!='date' and c not in master.columns:\n",
    "        out[c]=out[c].fillna(0)\n",
    "out.to_csv(MASTER_WITH_TOPICS,index=False)\n",
    "print('Saved',MASTER_WITH_TOPICS,'rows=',len(out),'cols=',len(out.columns))\n",
    "out.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
