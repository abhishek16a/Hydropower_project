{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6293afbc",
   "metadata": {},
   "source": [
    "# 03_text_topics (Project, with LDA per proposal)\n",
    "Create **project notices/news** (English + Nepali), derive **keyword features**, and perform **topic modelling (LDA)** as specified in the proposal. Finally, merge daily text features into engineered features.\n",
    "\n",
    "**Outputs**\n",
    "- `project_notices.csv` (created here)\n",
    "- `text_corpus.csv` (corpus used downstream)\n",
    "- `topics_daily.csv` (daily keyword counts + flags)\n",
    "- `lda_topics_daily.csv` (daily LDA topic proportions)\n",
    "- `master_with_topics.csv` (merged table for modelling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d1c4a",
   "metadata": {},
   "source": [
    "### Cell 1 — Setup & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Project window\n",
    "START_DATE = pd.Timestamp('2019-01-01')\n",
    "END_DATE   = pd.Timestamp('2023-12-31')\n",
    "\n",
    "# Files\n",
    "PROJECT_NOTICES = 'project_notices.csv'   # created in Cell 2\n",
    "TEXT_CORPUS_CSV = 'text_corpus.csv'       # used downstream\n",
    "TOPICS_DAILY_CSV = 'topics_daily.csv'\n",
    "LDA_TOPICS_DAILY_CSV = 'lda_topics_daily.csv'\n",
    "MASTER_PATH = 'features_daily.csv'        # your engineered daily features\n",
    "MASTER_WITH_TOPICS = 'master_with_topics.csv'\n",
    "\n",
    "print('Outputs will be saved next to this notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c037d",
   "metadata": {},
   "source": [
    "### Cell 2 — Build **project notices/news** corpus (2019–2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1613bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "def add(d, title, text):\n",
    "    rows.append({'date': d.isoformat(), 'title': title, 'text': text})\n",
    "\n",
    "# Phrase banks for varied but relevant content\n",
    "flood_lines = [\n",
    "    \"DHM alert: Heavy rain (मुसलधारे वर्षा) in Syangja/Kaski; high flow/flood (बाढी) risk along Kali Gandaki; warning (चेतावनी) issued.\",\n",
    "    \"Flood advisory: Tribeni rainfall > 80 mm/24h; downstream inundation (डुबान) risk; maintain spillway readiness.\",\n",
    "    \"River discharge rising rapidly; landslide (पहिरो) reported upstream; public सूचना to avoid river banks.\"\n",
    "]\n",
    "maint_lines = [\n",
    "    \"Scheduled maintenance (सम्भार/मर्मत) of Kaligandaki A Unit {u}; partial shutdown (बन्द) expected; generation impact 40–70 MW.\",\n",
    "    \"Gate inspection and lubrication; short outage (विद्युत अवरोध) in afternoon window; SCADA checks.\",\n",
    "    \"Sediment flushing and desilting at intake; debris removal near trash rack; safety protocol active.\"\n",
    "]\n",
    "outage_lines = [\n",
    "    \"Unplanned outage due to transformer trip; restoration in 2 hours; load shedding avoided via import/dispatch.\",\n",
    "    \"Breaker failure at switchyard; protection operated; controlled restart procedure underway.\",\n",
    "    \"Transient fault cleared; unit re-synchronized; monitoring vibration and temperature.\"\n",
    "]\n",
    "policy_lines = [\n",
    "    \"NEA policy (नीति) update on seasonal tariff and curtailment; provisions for import (आयात) in dry months, export (निर्यात) in surplus.\",\n",
    "    \"Directive (निर्देशन) on load management during peak hours; dispatch priority for run-of-river vs reservoir plants.\",\n",
    "    \"Regulation (विनियमन) note: dam safety inspection schedule and reporting requirements.\"\n",
    "]\n",
    "weather_lines = [\n",
    "    \"Weather watch: thunderstorm (मेघगर्जन) and strong winds (हावाहुरी) possible; short-duration heavy rain may spike inflow.\",\n",
    "    \"Monsoon onset declared; persistent precipitation (पानी) expected for 3–5 days; monitor reservoir level (जलस्तर).\",\n",
    "    \"Monsoon withdrawal likely next week; decreasing rainfall trend; plan reservoir refill strategy.\"\n",
    "]\n",
    "ops_lines = [\n",
    "    \"Dry season operation: low river discharge; reservoir near minimum level; prioritize peak-hour dispatch.\",\n",
    "    \"Controlled spilling initiated due to high inflow; downstream चेतावनी जारी; avoid river banks.\",\n",
    "    \"Turbine efficiency recovery plan after overhaul; performance test scheduled.\"\n",
    "]\n",
    "\n",
    "for y in range(2019, 2024):\n",
    "    # Dry-season ops\n",
    "    add(date(y,1,21), \"Dry season operation notice – low discharge\", ops_lines[0])\n",
    "    add(date(y,2,10), \"Reservoir refill & sediment management\", \"Reservoir refill plan; sediment (सिल्ट) monitoring; intake trash rack cleaning.\")\n",
    "\n",
    "    # Pre-monsoon maintenance (two windows)\n",
    "    add(date(y,3,12), f\"Planned maintenance outage at Kaligandaki A – Unit 1\", maint_lines[0].format(u=1))\n",
    "    add(date(y,4,18), f\"Planned maintenance outage at Kaligandaki A – Unit 2\", maint_lines[0].format(u=2))\n",
    "    add(date(y,5,9),  \"Gate inspection & lubrication\", maint_lines[1])\n",
    "\n",
    "    # Monsoon flood & weather alerts\n",
    "    add(date(y,6,25), \"DHM flood/weather bulletin – Gandaki basin\", flood_lines[0])\n",
    "    add(date(y,7,10), \"Flood advisory – Tribeni gauge rising\", flood_lines[1])\n",
    "    add(date(y,7,25), \"High silt load expected during storm\", maint_lines[2])\n",
    "    add(date(y,8,1),  \"Weather watch – thunderstorm activity\", weather_lines[0])\n",
    "    add(date(y,8,18), \"Flood advisory – controlled spilling at Kaligandaki A\", ops_lines[1])\n",
    "\n",
    "    # Unplanned outages\n",
    "    add(date(y,9,5),  \"Unplanned outage – transformer trip\", outage_lines[0])\n",
    "    add(date(y,10,7), \"Breaker failure at switchyard\", outage_lines[1])\n",
    "\n",
    "    # Policy/Directive updates\n",
    "    add(date(y,11,2), \"Policy update: seasonal generation & import management\", policy_lines[0])\n",
    "    add(date(y,12,15),\"Directive on peak load management\", policy_lines[1])\n",
    "\n",
    "# Extra varied events\n",
    "extras = [\n",
    "    (date(2020,7,20), \"Landslide blocking headrace – debris removal\",\n",
    "     \"Emergency: landslide (पहिरो) induced debris near intake; partial shutdown (बन्द) for 6 hours; rapid response team deployed.\"),\n",
    "    (date(2021,10,3), \"Gandaki basin weather watch – thunderstorm\", weather_lines[0]),\n",
    "    (date(2022,5,15), \"Planned turbine overhaul – Unit 2\",\n",
    "     \"Major overhaul (मर्मत) for efficiency recovery; expected completion 7 days; generation reduced by 70 MW.\"),\n",
    "    (date(2023,6,12), \"Monsoon onset – rainfall increase expected\", weather_lines[1]),\n",
    "]\n",
    "for d0, t, txt in extras:\n",
    "    add(d0, t, txt)\n",
    "\n",
    "project_df = pd.DataFrame(rows).sort_values('date').reset_index(drop=True)\n",
    "project_df.to_csv(PROJECT_NOTICES, index=False)\n",
    "print('Saved', PROJECT_NOTICES, '| rows =', len(project_df))\n",
    "project_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0363cb2",
   "metadata": {},
   "source": [
    "### Cell 3 — Select project window and save `text_corpus.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv(PROJECT_NOTICES, parse_dates=['date'])\n",
    "corpus_df = corpus_df[(corpus_df['date']>=START_DATE) & (corpus_df['date']<=END_DATE)]\n",
    "corpus_df = corpus_df.sort_values('date').reset_index(drop=True)\n",
    "corpus_df.to_csv(TEXT_CORPUS_CSV, index=False)\n",
    "print('Saved', TEXT_CORPUS_CSV, '| rows =', len(corpus_df))\n",
    "corpus_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d405d9a",
   "metadata": {},
   "source": [
    "### Cell 4 — Build daily keyword features → `topics_daily.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = {\n",
    "    \"maintenance\": [\"maintenance\",\"overhaul\",\"shutdown\",\"servicing\",\"repair\",\"gate inspection\",\"desilting\",\"trash rack\",\n",
    "                    \"सम्भार\",\"मर्मत\",\"सम्भाल\",\"बन्द\",\"नियमित जाँच\",\"सिल्ट\",\"सिल्टेशन\"],\n",
    "    \"outage\":      [\"outage\",\"blackout\",\"interruption\",\"trip\",\"fault\",\"breaker\",\"protection\",\"resynchroniz\",\n",
    "                    \"विद्युत अवरोध\",\"लोडसेडिङ\",\"बत्ती बन्द\",\"लोड व्यवस्थापन\"],\n",
    "    \"flood\":       [\"flood\",\"high flow\",\"inundation\",\"landslide\",\"debris\",\"spill\",\"spilling\",\"downstream\",\n",
    "                    \"बाढी\",\"पहिरो\",\"डुबान\",\"जोखिम\",\"चेतावनी\",\"सूचना\"],\n",
    "    \"policy\":      [\"policy\",\"tariff\",\"regulation\",\"directive\",\"order\",\"curtail\",\"import\",\"export\",\"dispatch\",\n",
    "                    \"नीति\",\"दर\",\"विनियमन\",\"निर्देशन\",\"आदेश\",\"आयात\",\"निर्यात\"],\n",
    "    \"weather\":     [\"heavy rain\",\"thunder\",\"storm\",\"wind\",\"monsoon\",\"precipitation\",\"rainfall\",\"forecast\",\n",
    "                    \"मुसलधारे वर्षा\",\"मेघगर्जन\",\"हावाहुरी\",\"मौसम\",\"पानी\"],\n",
    "}\n",
    "\n",
    "def kw_counts(text: str):\n",
    "    t = (text or \"\").lower()\n",
    "    out = {k: 0 for k in KEYWORDS}\n",
    "    for k, words in KEYWORDS.items():\n",
    "        out[k] = sum(t.count(w.lower()) for w in words)\n",
    "    return out\n",
    "\n",
    "rows = []\n",
    "for _, r in corpus_df.iterrows():\n",
    "    c = kw_counts(f\"{r.get('title','')} {r.get('text','')}\")\n",
    "    c[\"date\"] = r[\"date\"].date()\n",
    "    rows.append(c)\n",
    "\n",
    "daily_kw = pd.DataFrame(rows)\n",
    "daily_kw[\"date\"] = pd.to_datetime(daily_kw[\"date\"])\n",
    "\n",
    "for k in KEYWORDS:\n",
    "    daily_kw[f\"{k}_flag\"] = (daily_kw[k] > 0).astype(int)\n",
    "\n",
    "daily_kw = daily_kw.groupby(\"date\", as_index=False).sum().sort_values(\"date\")\n",
    "daily_kw.to_csv(TOPICS_DAILY_CSV, index=False)\n",
    "print(\"Saved\", TOPICS_DAILY_CSV, \"| rows =\", len(daily_kw))\n",
    "daily_kw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73b95d",
   "metadata": {},
   "source": [
    "### Cell 5 — Topic modelling (LDA) per proposal → `lda_topics_daily.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA on (title + text). We use English stopwords + a small Nepali stoplist.\n",
    "# If scikit-learn is missing, install it in your environment before running this cell.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Basic Nepali stoplist (extend as needed)\n",
    "nep_stop = [\n",
    "    \"छ\",\"देखि\",\"र\",\"को\",\"मा\",\"का\",\"को लागि\",\"यस\",\"भएको\",\"गरिएका\",\"गरिने\",\"सम्बन्धी\",\n",
    "    \"सूचना\",\"चेतावनी\",\"प्रेस\",\"विज्ञप्ति\",\"लगायत\",\"तथा\",\"प्रति\",\"नि\"\n",
    "]\n",
    "\n",
    "def build_corpus_text(df):\n",
    "    return (df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")).tolist()\n",
    "\n",
    "def make_vectorizer():\n",
    "    # Include bigrams to capture phrases like \"heavy rain\", \"load shedding\"\n",
    "    return CountVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=1,\n",
    "        ngram_range=(1,2),\n",
    "        stop_words='english'  # English stopwords\n",
    "    )\n",
    "\n",
    "texts = build_corpus_text(corpus_df)\n",
    "vec = make_vectorizer()\n",
    "X = vec.fit_transform(texts)\n",
    "\n",
    "# Remove a few Nepali stop tokens post-hoc by zeroing columns (simple)\n",
    "# (Optional refinement: custom tokenizer that drops Nepali stopwords before vectorization)\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "mask_keep = ~np.isin(vocab, nep_stop)\n",
    "X = X[:, mask_keep]\n",
    "vocab = vocab[mask_keep]\n",
    "\n",
    "# Fit LDA (choose topics K=5; adjust if you want finer granularity)\n",
    "K = 5\n",
    "lda = LatentDirichletAllocation(n_components=K, random_state=42, learning_method='online')\n",
    "W = lda.fit_transform(X)   # doc-topic weights (rows = documents)\n",
    "\n",
    "# Attach doc-topic weights back to corpus\n",
    "corpus_lda = corpus_df.copy()\n",
    "for k in range(K):\n",
    "    corpus_lda[f\"topic{k}\"] = W[:, k]\n",
    "\n",
    "# Aggregate to daily mean topic proportions\n",
    "topics_daily = corpus_lda.groupby(\"date\")[ [f\"topic{k}\" for k in range(K)] ].mean().reset_index().sort_values(\"date\")\n",
    "topics_daily.to_csv(LDA_TOPICS_DAILY_CSV, index=False)\n",
    "print(\"Saved\", LDA_TOPICS_DAILY_CSV, \"| rows =\", len(topics_daily))\n",
    "\n",
    "# Show top words per topic for interpretation\n",
    "def top_words(component, vocab, n=10):\n",
    "    idx = component.argsort()[-n:][::-1]\n",
    "    return [vocab[i] for i in idx]\n",
    "\n",
    "topic_words = {f\"topic{k}\": top_words(lda.components_[k], vocab, n=12) for k in range(K)}\n",
    "print(\"Top words per topic:\")\n",
    "for k, words in topic_words.items():\n",
    "    print(k, \"→\", \", \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12e37c",
   "metadata": {},
   "source": [
    "### Cell 6 — Merge daily keywords + LDA with engineered features → `master_with_topics.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5dcaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv(MASTER_PATH, parse_dates=[\"date\"])\n",
    "kw = pd.read_csv(TOPICS_DAILY_CSV, parse_dates=[\"date\"])\n",
    "lda_daily = pd.read_csv(LDA_TOPICS_DAILY_CSV, parse_dates=[\"date\"]) if Path(LDA_TOPICS_DAILY_CSV).exists() else pd.DataFrame(columns=[\"date\"])\n",
    "\n",
    "out = master.merge(kw, on=\"date\", how=\"left\")\n",
    "if not lda_daily.empty:\n",
    "    out = out.merge(lda_daily, on=\"date\", how=\"left\")\n",
    "\n",
    "# Fill new cols with 0 (keywords) and with daily means for topics where missing\n",
    "for c in out.columns:\n",
    "    if c not in master.columns and c != \"date\":\n",
    "        if c.startswith(\"topic\"):\n",
    "            out[c] = out[c].fillna(out[c].mean())\n",
    "        else:\n",
    "            out[c] = out[c].fillna(0)\n",
    "\n",
    "out.to_csv(MASTER_WITH_TOPICS, index=False)\n",
    "print(\"Saved\", MASTER_WITH_TOPICS, \"| rows:\", len(out), \"| cols:\", len(out.columns))\n",
    "out.head(5)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
