{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411eb6d7",
   "metadata": {},
   "source": [
    "# 03_text_topics_DHM_PDF_OCR\n",
    "Notebook to crawl DHM notice PDFs, extract text (with OCR fallback), build keyword features, and merge with master data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd14823",
   "metadata": {},
   "source": [
    "### Cell 1 — Setup & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, time, shutil\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "# Source (DHM notices)\n",
    "BASE = \"https://www.dhm.gov.np\"\n",
    "NOTICE_ROOT = f\"{BASE}/notice\"\n",
    "\n",
    "# Request settings\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "TIMEOUT = 30\n",
    "REQUEST_DELAY_SEC = 0.8\n",
    "\n",
    "# Project window\n",
    "START_DATE = pd.Timestamp(\"2019-01-01\")\n",
    "END_DATE   = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "# Paths\n",
    "RAW_DIR = Path(\"text_raw\"); RAW_DIR.mkdir(exist_ok=True)\n",
    "PDF_DIR = RAW_DIR / \"pdfs\"; PDF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Outputs\n",
    "TEXT_CORPUS_CSV = \"text_corpus.csv\"\n",
    "TOPICS_DAILY_CSV = \"topics_daily.csv\"\n",
    "MASTER_PATH = \"master_kaligandaki_daily_withrain.csv\"\n",
    "MASTER_WITH_TOPICS = \"master_with_topics.csv\"\n",
    "\n",
    "print(\"Dirs:\", RAW_DIR, PDF_DIR)\n",
    "print(\"Outputs:\", TEXT_CORPUS_CSV, TOPICS_DAILY_CSV, MASTER_WITH_TOPICS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd69b9",
   "metadata": {},
   "source": [
    "### Cell 2 — Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_html(url: str, sleep: float = REQUEST_DELAY_SEC):\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            r = requests.get(url, headers=UA, timeout=TIMEOUT)\n",
    "            if r.status_code == 200 and r.text and len(r.text) > 200:\n",
    "                time.sleep(sleep)\n",
    "                return r.text\n",
    "            else:\n",
    "                print(f\"[warn] status={r.status_code} len={len(r.text)} url={url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[retry {attempt+1}/3] GET failed: {url} -> {e}\")\n",
    "        time.sleep(1.0)\n",
    "    return \"\"\n",
    "\n",
    "def abs_url(href: str):\n",
    "    return urljoin(BASE + \"/\", href) if href else None\n",
    "\n",
    "def safe_date_from_text(txt: str):\n",
    "    if not txt: return None\n",
    "    try:\n",
    "        return dparser.parse(txt, fuzzy=True, dayfirst=True).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def notice_listing_urls(max_pages=30):\n",
    "    for p in range(max_pages):\n",
    "        yield f\"{NOTICE_ROOT}?page={p}\"\n",
    "        yield f\"{NOTICE_ROOT}/?page={p}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a32e1f",
   "metadata": {},
   "source": [
    "### Cell 3 — Crawl listings and collect PDF links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_links = set()\n",
    "\n",
    "for url in notice_listing_urls(max_pages=30):\n",
    "    html = get_html(url)\n",
    "    if not html:\n",
    "        continue\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = (a.get(\"href\") or \"\").strip()\n",
    "        if not href:\n",
    "            continue\n",
    "        if href.lower().endswith(\".pdf\") or \"pdf\" in href.lower():\n",
    "            full = abs_url(href)\n",
    "            if full and full.startswith(\"http\"):\n",
    "                pdf_links.add(full)\n",
    "    print(\"[list] collected so far:\", len(pdf_links))\n",
    "\n",
    "pdf_links = sorted(pdf_links)\n",
    "print(\"Total PDF links found:\", len(pdf_links))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb6a33",
   "metadata": {},
   "source": [
    "### Cell 4 — Download PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80827fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filename_from_url(u: str):\n",
    "    name = u.split(\"/\")[-1].split(\"?\")[0]\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", name or \"notice.pdf\")\n",
    "\n",
    "downloaded = []\n",
    "for i, url in enumerate(pdf_links, 1):\n",
    "    fn = filename_from_url(url)\n",
    "    path = PDF_DIR / fn\n",
    "    if path.exists() and path.stat().st_size > 500:\n",
    "        downloaded.append(path)\n",
    "        continue\n",
    "    try:\n",
    "        r = requests.get(url, headers=UA, timeout=TIMEOUT)\n",
    "        if r.status_code == 200 and r.content and len(r.content) > 500:\n",
    "            path.write_bytes(r.content)\n",
    "            downloaded.append(path)\n",
    "            print(f\"[pdf] {i}/{len(pdf_links)} saved ->\", path.name)\n",
    "        else:\n",
    "            print(f\"[skip] {url} (status={r.status_code}, size={len(r.content) if r.content else 0})\")\n",
    "    except Exception as e:\n",
    "        print(f\"[error] {url} ->\", e)\n",
    "    time.sleep(REQUEST_DELAY_SEC)\n",
    "\n",
    "print(\"Downloaded PDFs:\", len(downloaded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78283ff",
   "metadata": {},
   "source": [
    "### Cell 5 — PDF text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a648f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "    USE_PDFMINER = True\n",
    "except Exception:\n",
    "    USE_PDFMINER = False\n",
    "    try:\n",
    "        import PyPDF2\n",
    "    except Exception:\n",
    "        PyPDF2 = None\n",
    "\n",
    "def read_pdf_text(path: Path) -> str:\n",
    "    if USE_PDFMINER:\n",
    "        try:\n",
    "            return pdf_extract_text(str(path)) or \"\"\n",
    "        except Exception as e:\n",
    "            print(\"[pdfminer-fail]\", path.name, e)\n",
    "    if 'PyPDF2' in globals() and PyPDF2 is not None:\n",
    "        try:\n",
    "            text = []\n",
    "            with open(path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    try:\n",
    "                        text.append(page.extract_text() or \"\")\n",
    "                    except Exception:\n",
    "                        text.append(\"\")\n",
    "            return \"\\n\".join(text)\n",
    "        except Exception as e:\n",
    "            print(\"[PyPDF2-fail]\", path.name, e)\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8531d96",
   "metadata": {},
   "source": [
    "### Cell 6 — OCR-enabled extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac719af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "POPPLER_PATH = None\n",
    "TESSERACT_CMD = None\n",
    "if TESSERACT_CMD:\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD\n",
    "\n",
    "def extract_text_any(pdf_path: Path, max_pages_ocr=5) -> str:\n",
    "    # pdfminer\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text as _pdfminer_extract\n",
    "        txt = _pdfminer_extract(str(pdf_path)) or \"\"\n",
    "        if len(txt.strip()) >= 40:\n",
    "            return txt\n",
    "    except Exception:\n",
    "        pass\n",
    "    # PyPDF2\n",
    "    try:\n",
    "        import PyPDF2\n",
    "        text = []\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                try:\n",
    "                    t = page.extract_text() or \"\"\n",
    "                except Exception:\n",
    "                    t = \"\"\n",
    "                text.append(t)\n",
    "        txt = \"\\n\".join(text)\n",
    "        if len(txt.strip()) >= 40:\n",
    "            return txt\n",
    "    except Exception:\n",
    "        pass\n",
    "    # OCR\n",
    "    try:\n",
    "        images = convert_from_path(str(pdf_path), dpi=200, first_page=1, last_page=max_pages_ocr, poppler_path=POPPLER_PATH)\n",
    "        ocr_texts = [pytesseract.image_to_string(im) for im in images]\n",
    "        return \"\\n\".join(ocr_texts)\n",
    "    except Exception as e:\n",
    "        print(\"[OCR fail]\", pdf_path.name, e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1108c",
   "metadata": {},
   "source": [
    "### Cell 7 — Build text_corpus.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3692f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def guess_date_from_name(s: str):\n",
    "    pats = [\n",
    "        r\"(20\\d{2})[-_/\\.](\\d{1,2})[-_/\\.](\\d{1,2})\",\n",
    "        r\"(\\d{1,2})[-_/\\.](\\d{1,2})[-_/\\.](20\\d{2})\",\n",
    "    ]\n",
    "    for p in pats:\n",
    "        m = re.search(p, s)\n",
    "        if m:\n",
    "            g = [int(x) for x in m.groups()]\n",
    "            try:\n",
    "                if g[0] > 1900: y,mn,d = g\n",
    "                elif g[2] > 1900: d,mn,y = g\n",
    "                else: continue\n",
    "                return pd.to_datetime(f\"{y:04d}-{mn:02d}-{d:02d}\", errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.NaT\n",
    "\n",
    "records = []\n",
    "pdf_paths = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(\"PDFs found:\", len(pdf_paths))\n",
    "\n",
    "for i, path in enumerate(pdf_paths, 1):\n",
    "    txt = extract_text_any(path)\n",
    "    if not txt or len(txt.strip()) < 40:\n",
    "        continue\n",
    "    lines = [l.strip() for l in txt.splitlines() if l.strip()]\n",
    "    title = (lines[0] if lines else path.stem)[:160]\n",
    "    dt = safe_date_from_text(title) or safe_date_from_text(txt)\n",
    "    if dt:\n",
    "        dt_ts = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    else:\n",
    "        dt_ts = guess_date_from_name(path.stem)\n",
    "    records.append({\n",
    "        \"date\": dt_ts,\n",
    "        \"source\": \"DHM\",\n",
    "        \"title\": title,\n",
    "        \"text\": txt,\n",
    "        \"url\": path.as_posix()\n",
    "    })\n",
    "\n",
    "corpus_df = pd.DataFrame(records)\n",
    "if not corpus_df.empty:\n",
    "    corpus_df = corpus_df.dropna(subset=[\"date\"])\n",
    "    corpus_df[\"date\"] = pd.to_datetime(corpus_df[\"date\"]).dt.normalize()\n",
    "    mask = (corpus_df[\"date\"] >= START_DATE) & (corpus_df[\"date\"] <= END_DATE)\n",
    "    corpus_df = corpus_df.loc[mask]\n",
    "    corpus_df = corpus_df.drop_duplicates(subset=[\"date\",\"title\",\"url\"]).sort_values(\"date\")\n",
    "\n",
    "corpus_df.to_csv(TEXT_CORPUS_CSV, index=False)\n",
    "print(\"Saved:\", TEXT_CORPUS_CSV, \"| rows =\", len(corpus_df))\n",
    "corpus_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a296f",
   "metadata": {},
   "source": [
    "### Cell 8 — Keyword features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcde90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "KEYWORDS = {\n",
    "    \"maintenance\": [\"maintenance\",\"overhaul\",\"shutdown\",\"servicing\",\"repair\",\"सम्भार\",\"मर्मत\",\"बन्द\"],\n",
    "    \"outage\":      [\"outage\",\"blackout\",\"interruption\",\"load shedding\",\"trip\",\"fault\",\"विद्युत अवरोध\",\"लोडसेडिङ\",\"बत्ती बन्द\"],\n",
    "    \"flood\":       [\"flood\",\"high flow\",\"inundation\",\"alert\",\"warning\",\"watch\",\"बाढी\",\"पहिरो\",\"सूचना\",\"चेतावनी\"],\n",
    "    \"policy\":      [\"policy\",\"tariff\",\"import\",\"export\",\"regulation\",\"curtail\",\"नीति\",\"दर\",\"आयात\",\"निर्यात\",\"विनियमन\"],\n",
    "    \"weather\":     [\"heavy rain\",\"thunder\",\"storm\",\"monsoon\",\"precipitation\",\"हावाहुरी\",\"मुसलधारे\",\"मेघगर्जन\",\"मौसम\"],\n",
    "}\n",
    "\n",
    "def kw_counts(text: str):\n",
    "    t = (text or \"\").lower()\n",
    "    out = {k: 0 for k in KEYWORDS}\n",
    "    for k, words in KEYWORDS.items():\n",
    "        for w in words:\n",
    "            out[k] += t.count(w.lower())\n",
    "    return out\n",
    "\n",
    "if 'corpus_df' in globals() and len(corpus_df) > 0:\n",
    "    kk = []\n",
    "    for _, r in corpus_df.iterrows():\n",
    "        counts = kw_counts((r.get(\"title\") or \"\") + \" \" + (r.get(\"text\") or \"\"))\n",
    "        counts[\"date\"] = r[\"date\"].date()\n",
    "        kk.append(counts)\n",
    "    daily_kw = pd.DataFrame(kk)\n",
    "    daily_kw[\"date\"] = pd.to_datetime(daily_kw[\"date\"])\n",
    "    for k in KEYWORDS:\n",
    "        daily_kw[f\"{k}_flag\"] = (daily_kw[k] > 0).astype(int)\n",
    "    daily_kw = daily_kw.groupby(\"date\", as_index=False).sum()\n",
    "else:\n",
    "    daily_kw = pd.DataFrame(columns=[\"date\"] + list(KEYWORDS.keys()) + [f\"{k}_flag\" for k in KEYWORDS])\n",
    "\n",
    "daily_kw.to_csv(TOPICS_DAILY_CSV, index=False)\n",
    "print(\"Saved:\", TOPICS_DAILY_CSV, \"| rows =\", len(daily_kw))\n",
    "daily_kw.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02ed67",
   "metadata": {},
   "source": [
    "### Cell 9 — Merge with master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master = pd.read_csv(MASTER_PATH, parse_dates=[\"date\"])\n",
    "out = master.merge(daily_kw, on=\"date\", how=\"left\")\n",
    "for c in out.columns:\n",
    "    if c != \"date\" and c not in master.columns:\n",
    "        out[c] = out[c].fillna(0)\n",
    "out.to_csv(MASTER_WITH_TOPICS, index=False)\n",
    "print(\"Saved:\", MASTER_WITH_TOPICS, \"| rows =\", len(out), \"| cols =\", len(out.columns))\n",
    "out.head(10)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
