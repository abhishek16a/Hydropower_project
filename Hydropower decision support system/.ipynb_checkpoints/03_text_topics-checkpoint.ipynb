{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19d0f07",
   "metadata": {},
   "source": [
    "# 03 – Unstructured Text → Daily Topic/Keyword Features\n",
    "\n",
    "**Goal:** Collect and transform NEA notices & DHM bulletins/forecasts into daily signals for the BN/GP models.\n",
    "\n",
    "**Outputs:**\n",
    "- `text_corpus.csv` – raw parsed items with `date, source, title, text, url`\n",
    "- `topics_daily.csv` – daily features from keywords (and optional LDA topics)\n",
    "- `master_with_topics.csv` – join with your master/feature table by date\n",
    "\n",
    "**Notes:**\n",
    "- Requires internet for live scraping. If offline, place saved HTML into `text_raw/` then run parsing cells.\n",
    "- Start with **keyword flags** (maintenance/outage/flood/policy). Optional: LDA if `scikit-learn` is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 0. Imports & Config ====\n",
    "import os, re, json, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path('text_raw')\n",
    "RAW_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NEA_NOTICE_URLS = [\n",
    "    'https://www.nea.org.np/notice',\n",
    "    'https://www.nea.org.np/notifications'\n",
    "]\n",
    "DHM_URLS = [\n",
    "    'https://www.dhm.gov.np/bulletins',\n",
    "    'https://www.dhm.gov.np/meteorology-forecast/4'\n",
    "]\n",
    "\n",
    "TEXT_CORPUS_CSV = 'text_corpus.csv'\n",
    "TOPICS_DAILY_CSV = 'topics_daily.csv'\n",
    "MASTER_PATH = 'master_kaligandaki_daily_withrain.csv'   # or features_daily.csv\n",
    "MASTER_WITH_TOPICS = 'master_with_topics.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97075e",
   "metadata": {},
   "source": [
    "## 1) Fetch pages (live) or use saved HTML\n",
    "If offline, skip fetching and put HTML files into `text_raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db492d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 1. (Optional) Fetch ====\n",
    "try:\n",
    "    import requests\n",
    "    LIVE = True\n",
    "except Exception:\n",
    "    LIVE = False\n",
    "\n",
    "def fetch_and_save(url, outname):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        (RAW_DIR/outname).write_text(r.text, encoding='utf-8')\n",
    "        print('Saved', outname)\n",
    "    except Exception as e:\n",
    "        print('Fetch failed for', url, e)\n",
    "\n",
    "if LIVE:\n",
    "    for i,u in enumerate(NEA_NOTICE_URLS):\n",
    "        fetch_and_save(u, f'nea_{i}.html')\n",
    "    for i,u in enumerate(DHM_URLS):\n",
    "        fetch_and_save(u, f'dhm_{i}.html')\n",
    "else:\n",
    "    print('Offline mode: ensure RAW_DIR contains saved HTML files.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f9ecc",
   "metadata": {},
   "source": [
    "## 2) Parse HTML → corpus\n",
    "Flexible parsing with BeautifulSoup. Adjust selectors if sites change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a87b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2. Parse HTML into a corpus ====\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_items_from_html(html_text, source):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    items = []\n",
    "    for card in soup.select('article, .card, li, .list-item, .post, .notice, .news-item'):\n",
    "        text = card.get_text(' ', strip=True)\n",
    "        if not text or len(text) < 40:\n",
    "            continue\n",
    "        date_match = re.search(r'(\\d{1,2}\\s+[A-Za-z]{3,9}\\s+\\d{4}|\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4})', text)\n",
    "        dt = None\n",
    "        if date_match:\n",
    "            rawd = date_match.group(0)\n",
    "            for fmt in ('%d %b %Y','%d %B %Y','%Y-%m-%d','%d/%m/%Y'):\n",
    "                try:\n",
    "                    dt = datetime.strptime(rawd, fmt).date()\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "        title_tag = card.find(['h1','h2','h3'])\n",
    "        title = title_tag.get_text(' ', strip=True) if title_tag else text[:80]\n",
    "        url = None\n",
    "        a = card.find('a')\n",
    "        if a and a.has_attr('href'):\n",
    "            url = a['href']\n",
    "        items.append({'date': dt, 'source': source, 'title': title, 'text': text, 'url': url})\n",
    "    return items\n",
    "\n",
    "corpus = []\n",
    "for p in RAW_DIR.glob('nea_*.html'):\n",
    "    corpus += parse_items_from_html(p.read_text(encoding='utf-8', errors='ignore'), source='NEA')\n",
    "for p in RAW_DIR.glob('dhm_*.html'):\n",
    "    corpus += parse_items_from_html(p.read_text(encoding='utf-8', errors='ignore'), source='DHM')\n",
    "\n",
    "corpus_df = pd.DataFrame(corpus)\n",
    "corpus_df['date'] = pd.to_datetime(corpus_df['date'], errors='coerce')\n",
    "corpus_df = corpus_df.dropna(subset=['date']).sort_values('date')\n",
    "corpus_df.to_csv(TEXT_CORPUS_CSV, index=False)\n",
    "print('Saved:', TEXT_CORPUS_CSV, '| rows=', len(corpus_df))\n",
    "corpus_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f5fa0",
   "metadata": {},
   "source": [
    "## 3) Keyword features (fast & robust)\n",
    "Compute daily counts and binary flags for important keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab805b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3. Keywords → daily features ====\n",
    "KEYWORDS = {\n",
    "  'maintenance': ['maintenance','overhaul','shutdown','servicing','repair'],\n",
    "  'outage': ['outage','blackout','interruption','load shedding','trip','fault'],\n",
    "  'flood': ['flood','high flow','inundation','alert','warning','watch'],\n",
    "  'policy': ['policy','tariff','import','export','regulation','curtail'],\n",
    "  'weather': ['heavy rain','thunder','storm','monsoon','precipitation']\n",
    "}\n",
    "\n",
    "def keyword_counts(text: str, keywords: dict):\n",
    "    t = (text or '').lower()\n",
    "    out = {k: 0 for k in keywords}\n",
    "    for k, words in keywords.items():\n",
    "        for w in words:\n",
    "            out[k] += t.count(w)\n",
    "    return out\n",
    "\n",
    "kdf = []\n",
    "for _, row in corpus_df.iterrows():\n",
    "    counts = keyword_counts((row['title'] or '') + ' ' + (row['text'] or ''), KEYWORDS)\n",
    "    counts['date'] = row['date'].date()\n",
    "    kdf.append(counts)\n",
    "kdf = pd.DataFrame(kdf)\n",
    "kdf['date'] = pd.to_datetime(kdf['date'])\n",
    "daily_kw = kdf.groupby('date', as_index=False).sum()\n",
    "for k in KEYWORDS.keys():\n",
    "    daily_kw[f'{k}_flag'] = (daily_kw[k] > 0).astype(int)\n",
    "\n",
    "daily_kw.to_csv(TOPICS_DAILY_CSV, index=False)\n",
    "print('Saved:', TOPICS_DAILY_CSV, '| rows=', len(daily_kw))\n",
    "daily_kw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3eafb",
   "metadata": {},
   "source": [
    "## 4) (Optional) LDA topics\n",
    "If `scikit-learn` is installed, fit LDA to generate topic proportions per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 4. Optional: LDA ====\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    SK_OK = True\n",
    "except Exception:\n",
    "    SK_OK = False\n",
    "\n",
    "if SK_OK and len(corpus_df) > 10:\n",
    "    docs = (corpus_df['title'].fillna('') + ' ' + corpus_df['text'].fillna('')).tolist()\n",
    "    vec = CountVectorizer(max_features=5000, stop_words='english')\n",
    "    X = vec.fit_transform(docs)\n",
    "    lda = LatentDirichletAllocation(n_components=6, max_iter=20, random_state=42)\n",
    "    Theta = lda.fit_transform(X)\n",
    "    topics_df = pd.DataFrame(Theta, columns=[f'topic_{i}' for i in range(Theta.shape[1])])\n",
    "    topics_df['date'] = corpus_df['date'].values\n",
    "    daily_topics = topics_df.groupby(pd.to_datetime(topics_df['date']).dt.date).mean().reset_index()\n",
    "    daily_topics['date'] = pd.to_datetime(daily_topics['date'])\n",
    "else:\n",
    "    daily_topics = pd.DataFrame()\n",
    "\n",
    "daily_topics.head() if not daily_topics.empty else print('LDA skipped (no sklearn or too few docs).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c28e87",
   "metadata": {},
   "source": [
    "## 5) Merge with master\n",
    "Join keyword/topic features to your master table by `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4882167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 5. Merge ====\n",
    "master = pd.read_csv(MASTER_PATH, parse_dates=['date'])\n",
    "out = master.merge(pd.read_csv(TOPICS_DAILY_CSV, parse_dates=['date']), on='date', how='left')\n",
    "if not daily_topics.empty:\n",
    "    out = out.merge(daily_topics, on='date', how='left')\n",
    "out.to_csv('master_with_topics.csv', index=False)\n",
    "print('Saved: master_with_topics.csv | rows=', len(out))\n",
    "out.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
